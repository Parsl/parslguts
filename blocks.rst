.. index:: blocks, batch jobs

Blocks
######

In the task overview, I assumed that process worker pools magically existed in the right place: on the local machine with the example configuration, but on HPC worker nodes when running a more serious workflow.

The theme of this section is: how to get process worker pools running on the nodes where we want to do the work.

The configuration mechanisms talked about here are usually the most non-portable pieces of a Parsl workflow, because they are closely tied to the behaviour of particular HPC machines: this part of the configuration describes what an HPC machine looks like (as much as Parsl needs to know) and so the descriptions will be different for different machines. And so it's one of the most useful areas for admins and users to contribute documentation: for example, the Parsl user guide has a section with configurations for different machines, and ALCF and NERSC both maintain their own Parsl examples.

.. index:: pilot jobs

Pilot jobs
==========

Not all executors need workers - for example the ThreadPoolExecutor runs tasks within the main workflow process. But the HighThroughputExecutor as shown in the previous section, and also the Work Queue and Task Vine executors use workers running (for example) on the worker nodes of an HPC system. (This is also known as the pilot job model.)

We don't need to describe the work to be performed by the workers (at least, not much), because once the workers are running they'll get their own work from the interchange, as I talked about in the previous section. (that's another feature of the pilot job model)

Why are we doing things this way when an HPC system already has a system for running jobs (such as Slurm)? Because the overhead on that kind of job can be very big - those systems are targeted more at the scale of "run one job that uses 1000 nodes for 24 hours" but tasks in Parsl might be subsecond: even getting a new Python process started to run that subsecond task could be a prohibitive overhead.

As I mentioned above, most HPC systems have batch job systems that prefer big submissions (in relation to the average Parsl task) and that includes a preference for batch jobs that use many nodes (for example, some systems will offer a discount for batch jobs that use over a certain count - incentivising the use of a small number of many-node batch jobs, even though a pilot job workload could sometimes be scheduled more efficiently with a large number of smaller batch jobs)

.. index:: providers

Starting a block of workers
===========================

assume we're using an executor configured to use parsl scaling (it doesn't have to be this way)

Parsl is going to decide it wants some workers (how and why, see the upcoming scaling section). For now assume it has decided it wants some.

.. index:: blocks

The unit of allocation of workers in Parsl is called a block.

This translates into whatever the underlying batch system uses for allocating workers: for exampl,e with traditional supercomputing systems, the ``SlurmProvider`` will make 1 block = 1 Slurm batch job; the ``KubernetesProvider`` will make 1 block = 1 pod; and with the ``LocalProvider``, there is no meaningful allocation of jobs and the provider will run the worker directly on the local machine.

The base class for all providers is ``ExecutionProvider`` https://github.com/Parsl/parsl/blob/3f2bf1865eea16cc44d6b7f8938a1ae1781c61fd/parsl/providers/base.py#L11

As far as getting a new block of workers running, this is the most important method that a concrete provider must implement:

.. code-block:: python

     @abstractmethod
     def submit(self, command: str, tasks_per_node: int, job_name: str = "parsl.auto") -> object:
       ...

The key argument here is ``command``. This will be (after some mutation) be the Unix shell command (generated by some other part of Parsl) that should be run on each allocated node to start the workers.

In the HighThroughputExecutor, this command is formed like this at executor start:

TODO

and then the provider is invoked with it here:

TODO

In the Task Vine executor, something similar happens at line TODO and line TODO (hrefs)


.. note::
     WART: tasks_per_node is always 1 here when called by Parsl. It should perhaps be removed. It's a vestige of an earlier time when Parsl wanted the batch system to start multiple workers on each worker node (for the long-removed IPyParallel executor). More recent executors, the HighThroughputExecutor, the WorkQueue and TaskVine executor and the MPIExecutor choose to manage (in different ways) how work is performed on a particular node rather than asking the batch system for a particular fixed number of workers.

Maybe interesting here is what is missing from the ``submit`` call: there is no mention of batch system queues, no mention of how many nodes to request in this block, no mention of pod image identifiers.


* LRM providers

* launchers (not all providers use these, so provider is more "fundamental" - specifically its the providers that fit the HPC supercomputer model, vs eg. kubernetes or the cloud-like providers)
  note that in some batch systems, the batch script doesnt' run on a worker node but on a separate management node, and anything big/serious should be launched with something like mpiexec or aprun - so that those things run on the allocated worker nodes.

Choosing when to start or end a block
=====================================

* scaling strategies and error handling (two parts of the same feedback loop)

Worker environments
===================

* batch job environments (esp worker_init) - think about parsl requirements a bit more: Python versions, Parsl versions, installed user packages. forward reference serialization chapter.

batch job systems generally won't make the environment that your batch job providers look like the environment the submission comes from (in the case of eg. kubernetes, that's very deliberate: the job description describes the environment, not whatever ambient environment existing around the submission command. so there's a bit of tension there when you want the environment to magically look like your submission environment)

